---
title: Denoising Diffusion Probabilistic Models
date: 2025-10-26
category: Diffusion
image: static/img/DDPMtn.png
summary: "Recently, there are many generative models using diffusion model. We can study from the start of diffusion model, DDPM!"
---

Before reading this post, You can **think** about this questions.

> " If we intentionally destroy data by adding noise (diffusion), can we really train a model to *reverse* the process?"

> "And in doing so, Is it possible to create *high-quality* new samples from pure noise? What does this process mean in terms of *information theory perspective*?"

ðŸ““ **Paper** : Jonathan Ho, Ajay Jain, Pieter Abbeel (2020). Denoising Diffusion Probabilistic Models

ðŸ“š **Additional Reading materials (ex. Blog)** : Lilian Weng, What are Diffusion Models? (For broader context, acceleration, and guidance methods)

---

# I. SHORT SUMMARY

**Denoising Diffusion Probabilistic Models (DDPMs)** are a generative modeling framework based on a parameterized Markov chain. The model learns to reverse a fixed forward (diffusion) process, which gradually adds Gaussian noise to data until it becomes pure noise. The reverse chain starts from a standard Gaussian prior and learns to progressively denoise it back into a sample. 

This model's core innovation is parameterizing the reverse process to predict the noise at each step and simplifying the complex training objective function into a simple objective function. 


# II. MATHEMATICAL BACKGROUND 

This concepts are general concepts to understand this paper.

## II-1) 1st-Order Markov Process

A first-order Markov process (or Markov chain) is a stochastic model describing a sequence of possible events. 

The key property of the "Markov property" is that the probability of the next state depends only on the current state, not on the sequence of events that preceded it. 

$$p(x_t | x_{t-1}, x_{t-2}, ..., x_0) = p(x_t | x_{t-1})$$

This "memoryless" property dramatically simplifies the joint probability of a long chain, allowing it to be factored into a product of transition probabilities.

By joint probability (Chain Rule), This property allows the joint probability of an entire sequence to be factored into a product of conditional probabilities. This is fundamental to defining both the forward and reverse processes in DDPM.

$$
p(x_{0:T}) = p(x_0)\prod_{t=1}^{T}p(x_t \mid x_{t-1})
$$


## II-2) Evidence Lower Bound (ELBO)

In variational inference (VI), the data log-likelihood $log p_{\theta}(x)$ is often intractable.

The ELBO (Evidence Lower Bound) provides a computable lower bound. 

By introducing an approximate posterior $q(z|x)$ and applying Jensen's inequality,
We can get following bounds.

$$
\log p_{\theta}(x)
= \log \int p_{\theta}(x, z)\ dz
= \log \int q(z \mid x) \frac{p_{\theta}(x, z)}{q(z \mid x)}\ dz
\ge \int q(z \mid x) \log \frac{p_{\theta}(x, z)}{q(z \mid x)}\ dz 
$$ 


So, The ELBO is typically decomposed into two terms.


$$
\text{ELBO}
= \mathbb{E}_{q(z \mid x)} \left[ \log p_{\theta}(x \mid z) \right] - D_{KL}\!\left( q(z \mid x) \,\|\, p_{\theta}(z) \right)
$$

## II-3) KL Divergence of Gaussian

In this paper (DDPM), we assume Gaussian Distributions for both the forward and reverse processes. 
This means that the terms in the ELBO often reduce to computing the KL divergence between two Gaussians. 

For two multivariate normal distribution $p$ and $q$, the KL Divergence has a closed-form solution.

$$
D_{KL}(q \| p)
= \sum_{i} \left(
\log \frac{\sigma_{p}}{\sigma_{q}}
+ \frac{\sigma_{q}^{2} + (\mu_{q} - \mu_{p})^{2}}{2\sigma_{p}^{2}}
- \frac{1}{2}
\right)
$$


# III. Diffusion Model

A DDPM is a latent variable model by Forward Process and Reverse Process.

## III-1) Forward and Reverse Process

![Figure 1. Forward process and reverse process](static/img/DDPM/DDPM.png)

Forward and reverse process are both Markovian processes.

- Forward Process $q$ (Diffusion) : This is a fixed process with no learnable parameters that gradually adds Gaussian noise to an image $x_0$ over $T$ steps according to a variance schedule $\beta_t$.

$$q(x_t|x_{t-1}) := \mathcal{N}(x_t; \sqrt{1-\beta_t}x_{t-1}, \beta_t\mathbf{I})$$

A key property is that we can sample $x_t$ at any arbitrary timestep directly from $x_0$. Defining $\alpha_t := 1 - \beta_t$ and $\bar{\alpha}_t := \prod_{s=1}^t \alpha_s$, we have the closed form.

$$q(x_t|x_0) = \mathcal{N}(x_t; \sqrt{\bar{\alpha}_t}x_0, (1-\bar{\alpha}_t)\mathbf{I})$$ 

 This makes training highly efficient, as we can sample a random $t$ for each training example.

- Reverse Process $p_\theta$ (Denoising) : This is a **learned** Markov chain that aims to reverse the diffusion, starting from pure noise $x_T \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$ and progressively generating a data sample.
    
 $$p_\theta(x_{t-1}|x_t) := \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \mathbf{\Sigma}_\theta(x_t, t))$$
 
 
## III-2. Loss Function

This model is trained by optimizing the variational 




